{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-08T14:41:58.137199Z","iopub.execute_input":"2024-02-08T14:41:58.137630Z","iopub.status.idle":"2024-02-08T14:41:58.654520Z","shell.execute_reply.started":"2024-02-08T14:41:58.137600Z","shell.execute_reply":"2024-02-08T14:41:58.653230Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"Function defination for activation functions","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\ndef softmax(x):\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum(axis=0)\n\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))","metadata":{"execution":{"iopub.status.busy":"2024-02-08T14:42:06.402751Z","iopub.execute_input":"2024-02-08T14:42:06.403203Z","iopub.status.idle":"2024-02-08T14:42:06.430629Z","shell.execute_reply.started":"2024-02-08T14:42:06.403166Z","shell.execute_reply":"2024-02-08T14:42:06.429044Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"Functions for unit test cases","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\ndef rnn_cell_forward_tests(target):\n    # Only bias in expression\n    a_prev_tmp = np.zeros((5, 10))\n    xt_tmp = np.zeros((3, 10))\n    parameters_tmp = {}\n    parameters_tmp['Waa'] = np.random.randn(5, 5)\n    parameters_tmp['Wax'] = np.random.randn(5, 3)\n    parameters_tmp['Wya'] = np.random.randn(2, 5)\n    parameters_tmp['ba'] = np.random.randn(5, 1)\n    parameters_tmp['by'] = np.random.randn(2, 1)\n    parameters_tmp['Wya'] = np.zeros((2, 5))\n\n    a_next_tmp, yt_pred_tmp, cache_tmp = target(xt_tmp, a_prev_tmp, parameters_tmp)\n    \n    assert a_next_tmp.shape == (5, 10), f\"Wrong shape for a_next. Expected (5, 10) != {a_next_tmp.shape}\"\n    assert yt_pred_tmp.shape == (2, 10), f\"Wrong shape for yt_pred. Expected (2, 10) != {yt_pred_tmp.shape}\"\n    assert cache_tmp[0].shape == (5, 10), \"Wrong shape in cache->a_next\"\n    assert cache_tmp[1].shape == (5, 10), \"Wrong shape in cache->a_prev\"\n    assert cache_tmp[2].shape == (3, 10), \"Wrong shape in cache->x_t\"\n    assert len(cache_tmp[3].keys()) == 5, \"Wrong number of parameters in cache. Expected 5\"\n    \n    assert np.allclose(np.tanh(parameters_tmp['ba']), a_next_tmp), \"Problem 1 in a_next expression. Related to ba?\"\n    assert np.allclose(softmax(parameters_tmp['by']), yt_pred_tmp), \"Problem 1 in yt_pred expression. Related to by?\"\n\n    # Only xt in expression\n    a_prev_tmp = np.zeros((5,10))\n    xt_tmp = np.random.randn(3,10)\n    parameters_tmp['Wax'] = np.random.randn(5,3)\n    parameters_tmp['ba'] = np.zeros((5,1))\n    parameters_tmp['by'] = np.zeros((2,1))\n\n    a_next_tmp, yt_pred_tmp, cache_tmp = target(xt_tmp, a_prev_tmp, parameters_tmp)\n\n    assert np.allclose(np.tanh(np.dot(parameters_tmp['Wax'], xt_tmp)), a_next_tmp), \"Problem 2 in a_next expression. Related to xt?\"\n    assert np.allclose(softmax(np.dot(parameters_tmp['Wya'], a_next_tmp)), yt_pred_tmp), \"Problem 2 in yt_pred expression. Related to a_next?\"\n\n    # Only a_prev in expression\n    a_prev_tmp = np.random.randn(5,10)\n    xt_tmp = np.zeros((3,10))\n    parameters_tmp['Waa'] = np.random.randn(5,5)\n    parameters_tmp['ba'] = np.zeros((5,1))\n    parameters_tmp['by'] = np.zeros((2,1))\n\n    a_next_tmp, yt_pred_tmp, cache_tmp = target(xt_tmp, a_prev_tmp, parameters_tmp)\n\n    assert np.allclose(np.tanh(np.dot(parameters_tmp['Waa'], a_prev_tmp)), a_next_tmp), \"Problem 3 in a_next expression. Related to a_prev?\"\n    assert np.allclose(softmax(np.dot(parameters_tmp['Wya'], a_next_tmp)), yt_pred_tmp), \"Problem 3 in yt_pred expression. Related to a_next?\"\n\n    print(\"\\033[92mAll tests passed\")\n    \n\ndef rnn_forward_test(target):\n    np.random.seed(17)\n    T_x = 13\n    m = 8\n    n_x = 4\n    n_a = 7\n    n_y = 3\n    x_tmp = np.random.randn(n_x, m, T_x)\n    a0_tmp = np.random.randn(n_a, m)\n    parameters_tmp = {}\n    parameters_tmp['Waa'] = np.random.randn(n_a, n_a)\n    parameters_tmp['Wax'] = np.random.randn(n_a, n_x)\n    parameters_tmp['Wya'] = np.random.randn(n_y, n_a)\n    parameters_tmp['ba'] = np.random.randn(n_a, 1)\n    parameters_tmp['by'] = np.random.randn(n_y, 1)\n\n    a, y_pred, caches = target(x_tmp, a0_tmp, parameters_tmp)\n    \n    assert a.shape == (n_a, m, T_x), f\"Wrong shape for a. Expected: ({n_a, m, T_x}) != {a.shape}\"\n    assert y_pred.shape == (n_y, m, T_x), f\"Wrong shape for y_pred. Expected: ({n_y, m, T_x}) != {y_pred.shape}\"\n    assert len(caches[0]) == T_x, f\"len(cache) must be T_x = {T_x}\"\n    \n    assert np.allclose(a[5, 2, 2:6], [0.99999291, 0.99332189, 0.9921928, 0.99503445]), \"Wrong values for a\"\n    assert np.allclose(y_pred[2, 1, 1: 5], [0.19428, 0.14292, 0.24993, 0.00119], atol=1e-4), \"Wrong values for y_pred\"\n    assert np.allclose(caches[1], x_tmp), f\"Fail check: cache[1] != x_tmp\"\n\n    \n    print(\"\\033[92mAll tests passed\")\n    \ndef lstm_cell_forward_test(target):\n    np.random.seed(212)\n    m = 8\n    n_x = 4\n    n_a = 7\n    n_y = 3\n    x = np.random.randn(n_x, m)\n    a0 = np.random.randn(n_a, m)\n    c0 = np.random.randn(n_a, m)\n    params = {}\n    params['Wf'] = np.random.randn(n_a, n_a + n_x)\n    params['bf'] = np.random.randn(n_a, 1)\n    params['Wi'] = np.random.randn(n_a, n_a + n_x)\n    params['bi'] = np.random.randn(n_a, 1)\n    params['Wo'] = np.random.randn(n_a, n_a + n_x)\n    params['bo'] = np.random.randn(n_a, 1)\n    params['Wc'] = np.random.randn(n_a, n_a + n_x)\n    params['bc'] = np.random.randn(n_a, 1)\n    params['Wy'] = np.random.randn(n_y, n_a)\n    params['by'] = np.random.randn(n_y, 1)\n    a_next, c_next, y_pred, cache = target(x, a0, c0, params)\n    \n    assert len(cache) == 10, \"Don't change the cache\"\n    \n    assert cache[4].shape == (n_a, m), f\"Wrong shape for cache[4](ft). {cache[4].shape} != {(n_a, m)}\"\n    assert cache[5].shape == (n_a, m), f\"Wrong shape for cache[5](it). {cache[5].shape} != {(n_a, m)}\"\n    assert cache[6].shape == (n_a, m), f\"Wrong shape for cache[6](cct). {cache[6].shape} != {(n_a, m)}\"\n    assert cache[1].shape == (n_a, m), f\"Wrong shape for cache[1](c_next). {cache[1].shape} != {(n_a, m)}\"\n    assert cache[7].shape == (n_a, m), f\"Wrong shape for cache[7](ot). {cache[7].shape} != {(n_a, m)}\"\n    assert cache[0].shape == (n_a, m), f\"Wrong shape for cache[0](a_next). {cache[0].shape} != {(n_a, m)}\"\n    assert cache[8].shape == (n_x, m), f\"Wrong shape for cache[8](xt). {cache[8].shape} != {(n_x, m)}\"\n    assert cache[2].shape == (n_a, m), f\"Wrong shape for cache[2](a_prev). {cache[2].shape} != {(n_a, m)}\"\n    assert cache[3].shape == (n_a, m), f\"Wrong shape for cache[3](c_prev). {cache[3].shape} != {(n_a, m)}\"\n    \n    assert a_next.shape == (n_a, m), f\"Wrong shape for a_next. {a_next.shape} != {(n_a, m)}\"\n    assert c_next.shape == (n_a, m), f\"Wrong shape for c_next. {c_next.shape} != {(n_a, m)}\"\n    assert y_pred.shape == (n_y, m), f\"Wrong shape for y_pred. {y_pred.shape} != {(n_y, m)}\"\n\n    \n    assert np.allclose(cache[4][0, 0:2], [0.32969833, 0.0574555]), \"wrong values for ft\"\n    assert np.allclose(cache[5][0, 0:2], [0.0036446, 0.9806943]), \"wrong values for it\"\n    assert np.allclose(cache[6][0, 0:2], [0.99903873, 0.57509956]), \"wrong values for cct\"\n    assert np.allclose(cache[1][0, 0:2], [0.1352798,  0.39884899]), \"wrong values for c_next\"\n    assert np.allclose(cache[7][0, 0:2], [0.7477249,  0.71588751]), \"wrong values for ot\"\n    assert np.allclose(cache[0][0, 0:2], [0.10053951, 0.27129536]), \"wrong values for a_next\"\n    \n    assert np.allclose(y_pred[1], [0.417098, 0.449528, 0.223159, 0.278376,\n                                   0.68453,  0.419221, 0.564025, 0.538475]), \"Wrong values for y_pred\"\n    \n    print(\"\\033[92mAll tests passed\")\n    \ndef lstm_forward_test(target):\n    np.random.seed(45)\n    n_x = 4\n    m = 13\n    T_x = 16\n    n_a = 3\n    n_y = 2\n    x_tmp = np.random.randn(n_x, m, T_x)\n    a0_tmp = np.random.randn(n_a, m)\n    parameters_tmp = {}\n    parameters_tmp['Wf'] = np.random.randn(n_a, n_a + n_x)\n    parameters_tmp['bf'] = np.random.randn(n_a, 1)\n    parameters_tmp['Wi'] = np.random.randn(n_a, n_a + n_x)\n    parameters_tmp['bi']= np.random.randn(n_a, 1)\n    parameters_tmp['Wo'] = np.random.randn(n_a, n_a + n_x)\n    parameters_tmp['bo'] = np.random.randn(n_a, 1)\n    parameters_tmp['Wc'] = np.random.randn(n_a, n_a + n_x)\n    parameters_tmp['bc'] = np.random.randn(n_a, 1)\n    parameters_tmp['Wy'] = np.random.randn(n_y, n_a)\n    parameters_tmp['by'] = np.random.randn(n_y, 1)\n\n    a, y, c, caches = target(x_tmp, a0_tmp, parameters_tmp)\n    \n    assert a.shape == (n_a, m, T_x), f\"Wrong shape for a. {a.shape} != {(n_a, m, T_x)}\"\n    assert c.shape == (n_a, m, T_x), f\"Wrong shape for c. {c.shape} != {(n_a, m, T_x)}\"\n    assert y.shape == (n_y, m, T_x), f\"Wrong shape for y. {y.shape} != {(n_y, m, T_x)}\"\n    assert len(caches[0]) == T_x, f\"Wrong shape for caches. {len(caches[0])} != {T_x} \"\n    assert len(caches[0][0]) == 10, f\"length of caches[0][0] must be 10.\"\n    \n    assert np.allclose(a[2, 1, 4:6], [-0.01606022,  0.0243569]), \"Wrong values for a\"\n    assert np.allclose(c[2, 1, 4:6], [-0.02753855,  0.05668358]), \"Wrong values for c\"\n    assert np.allclose(y[1, 1, 4:6], [0.70444592 ,0.70648935]), \"Wrong values for y\"\n    \n    print(\"\\033[92mAll tests passed\")","metadata":{"execution":{"iopub.status.busy":"2024-02-08T14:42:17.005820Z","iopub.execute_input":"2024-02-08T14:42:17.006310Z","iopub.status.idle":"2024-02-08T14:42:17.058746Z","shell.execute_reply.started":"2024-02-08T14:42:17.006276Z","shell.execute_reply":"2024-02-08T14:42:17.057491Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"RNN Single Cell Structure (Forward)","metadata":{}},{"cell_type":"code","source":"# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: rnn_cell_forward\n\ndef rnn_cell_forward(xt, a_prev, parameters):\n    \"\"\"\n    Implements a single forward step of the RNN-cell as described in Figure (2)\n\n    Arguments:\n    xt -- your input data at timestep \"t\", numpy array of shape (n_x, m).\n    a_prev -- Hidden state at timestep \"t-1\", numpy array of shape (n_a, m)\n    parameters -- python dictionary containing:\n                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n                        ba --  Bias, numpy array of shape (n_a, 1)\n                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n    Returns:\n    a_next -- next hidden state, of shape (n_a, m)\n    yt_pred -- prediction at timestep \"t\", numpy array of shape (n_y, m)\n    cache -- tuple of values needed for the backward pass, contains (a_next, a_prev, xt, parameters)\n    \"\"\"\n        # Retrieve parameters from \"parameters\"\n    Wax = parameters[\"Wax\"]\n    Waa = parameters[\"Waa\"]\n    Wya = parameters[\"Wya\"]\n    ba = parameters[\"ba\"]\n    by = parameters[\"by\"]\n    \n    ### START CODE HERE ### (≈2 lines)\n    # compute next activation state using the formula given above\n    #print(np.dot(Waa,a_prev))\n    #print(np.dot(Wax,xt))\n    \n    a_next = np.tanh(np.dot(Waa,a_prev) + np.dot(Wax,xt) + ba)\n    #print(a_next)\n    # compute output of the current cell using the formula given above\n    yt_pred = softmax( np.dot(Wya,a_next) + by)\n    #print(np.dot(Wya,a_next))\n    #print(yt_pred)\n    ### END CODE HERE ###\n    \n    # store values you need for backward propagation in cache\n    cache = (a_next, a_prev, xt, parameters)\n    \n    return a_next, yt_pred, cache","metadata":{"execution":{"iopub.status.busy":"2024-02-08T15:14:52.683125Z","iopub.execute_input":"2024-02-08T15:14:52.683628Z","iopub.status.idle":"2024-02-08T15:14:52.693927Z","shell.execute_reply.started":"2024-02-08T15:14:52.683590Z","shell.execute_reply":"2024-02-08T15:14:52.692930Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"np.random.seed(1)\nxt_tmp = np.random.randn(3, 10)\na_prev_tmp = np.random.randn(5, 10)\nparameters_tmp = {}\nparameters_tmp['Waa'] = np.random.randn(5, 5)\nparameters_tmp['Wax'] = np.random.randn(5, 3)\nparameters_tmp['Wya'] = np.random.randn(2, 5)\nparameters_tmp['ba'] = np.random.randn(5, 1)\nparameters_tmp['by'] = np.random.randn(2, 1)","metadata":{"execution":{"iopub.status.busy":"2024-02-08T14:51:23.833475Z","iopub.execute_input":"2024-02-08T14:51:23.834232Z","iopub.status.idle":"2024-02-08T14:51:23.843876Z","shell.execute_reply.started":"2024-02-08T14:51:23.834195Z","shell.execute_reply":"2024-02-08T14:51:23.842605Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"a_next_tmp, yt_pred_tmp, cache_tmp = rnn_cell_forward(xt_tmp, a_prev_tmp, parameters_tmp)\nprint(\"a_next[4] = \\n\", a_next_tmp[4])\nprint(\"a_next.shape = \\n\", a_next_tmp.shape)\nprint(\"yt_pred[1] =\\n\", yt_pred_tmp[1])\nprint(\"yt_pred.shape = \\n\", yt_pred_tmp.shape)\n\n# UNIT TESTS\nrnn_cell_forward_tests(rnn_cell_forward)","metadata":{"execution":{"iopub.status.busy":"2024-02-08T15:14:54.951628Z","iopub.execute_input":"2024-02-08T15:14:54.952915Z","iopub.status.idle":"2024-02-08T15:14:54.963316Z","shell.execute_reply.started":"2024-02-08T15:14:54.952868Z","shell.execute_reply":"2024-02-08T15:14:54.961927Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"a_next[4] = \n [ 0.59584544  0.18141802  0.61311866  0.99808218  0.85016201  0.99980978\n -0.18887155  0.99815551  0.6531151   0.82872037]\na_next.shape = \n (5, 10)\nyt_pred[1] =\n [0.9888161  0.01682021 0.21140899 0.36817467 0.98988387 0.88945212\n 0.36920224 0.9966312  0.9982559  0.17746526]\nyt_pred.shape = \n (2, 10)\n\u001b[92mAll tests passed\n","output_type":"stream"}]},{"cell_type":"markdown","source":"RNN Network : Forward Propogation","metadata":{}},{"cell_type":"code","source":"# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: rnn_forward\n\ndef rnn_forward(x, a0, parameters):\n    \"\"\"\n    Implement the forward propagation of the recurrent neural network described in Figure (3).\n\n    Arguments:\n    x -- Input data for every time-step, of shape (n_x, m, T_x).\n    a0 -- Initial hidden state, of shape (n_a, m)\n    parameters -- python dictionary containing:\n                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n                        ba --  Bias numpy array of shape (n_a, 1)\n                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n\n    Returns:\n    a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)\n    y_pred -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)\n    caches -- tuple of values needed for the backward pass, contains (list of caches, x)\n    \"\"\"\n    \n    # Initialize \"caches\" which will contain the list of all caches\n    caches = []\n    \n    # Retrieve dimensions from shapes of x and parameters[\"Wya\"]\n    n_x, m, T_x = x.shape\n    n_y, n_a = parameters[\"Wya\"].shape\n    ### START CODE HERE ###\n    \n    # initialize \"a\" and \"y_pred\" with zeros (≈2 lines)\n    a=np.zeros((n_a,m,T_x))\n    y_pred=np.zeros((n_y,m,T_x))\n    \n    \n    # Initialize a_next (≈1 line)\n    a_next = a0\n    \n    # loop over all time-steps\n    for t in range(T_x):\n        # Update next hidden state, compute the prediction, get the cache (≈1 line)\n        a_next, yt_pred, cache = rnn_cell_forward(x[:,:,t], a_next, parameters)\n        # Save the value of the new \"next\" hidden state in a (≈1 line)\n        a[:,:,t] = a_next\n        # Save the value of the prediction in y (≈1 line)\n        y_pred[:,:,t] = yt_pred\n        # Append \"cache\" to \"caches\" (≈1 line)\n        caches.append(cache)\n        \n    ### END CODE HERE ###\n    \n    # store values needed for backward propagation in cache\n    caches = (caches, x)\n    \n    return a, y_pred, caches","metadata":{"execution":{"iopub.status.busy":"2024-02-08T16:24:31.647089Z","iopub.execute_input":"2024-02-08T16:24:31.647498Z","iopub.status.idle":"2024-02-08T16:24:31.660989Z","shell.execute_reply.started":"2024-02-08T16:24:31.647468Z","shell.execute_reply":"2024-02-08T16:24:31.659949Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"np.random.seed(1)\nx_tmp = np.random.randn(3, 10, 4)\na0_tmp = np.random.randn(5, 10)\nparameters_tmp = {}\nparameters_tmp['Waa'] = np.random.randn(5, 5)\nparameters_tmp['Wax'] = np.random.randn(5, 3)\nparameters_tmp['Wya'] = np.random.randn(2, 5)\nparameters_tmp['ba'] = np.random.randn(5, 1)\nparameters_tmp['by'] = np.random.randn(2, 1)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-08T16:24:37.027968Z","iopub.execute_input":"2024-02-08T16:24:37.029238Z","iopub.status.idle":"2024-02-08T16:24:37.037515Z","shell.execute_reply.started":"2024-02-08T16:24:37.029192Z","shell.execute_reply":"2024-02-08T16:24:37.036199Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"a_tmp, y_pred_tmp, caches_tmp = rnn_forward(x_tmp, a0_tmp, parameters_tmp)\nprint(\"a[4][1] = \\n\", a_tmp[4][1])\nprint(\"a.shape = \\n\", a_tmp.shape)\nprint(\"y_pred[1][3] =\\n\", y_pred_tmp[1][3])\nprint(\"y_pred.shape = \\n\", y_pred_tmp.shape)\nprint(\"caches[1][1][3] =\\n\", caches_tmp[1][1][3])\nprint(\"len(caches) = \\n\", len(caches_tmp))\n\n#UNIT TEST    \nrnn_forward_test(rnn_forward)","metadata":{"execution":{"iopub.status.busy":"2024-02-08T16:24:48.480055Z","iopub.execute_input":"2024-02-08T16:24:48.481448Z","iopub.status.idle":"2024-02-08T16:24:48.493096Z","shell.execute_reply.started":"2024-02-08T16:24:48.481401Z","shell.execute_reply":"2024-02-08T16:24:48.491731Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"a[4][1] = \n [-0.99999375  0.77911235 -0.99861469 -0.99833267]\na.shape = \n (5, 10, 4)\ny_pred[1][3] =\n [0.79560373 0.86224861 0.11118257 0.81515947]\ny_pred.shape = \n (2, 10, 4)\ncaches[1][1][3] =\n [-1.1425182  -0.34934272 -0.20889423  0.58662319]\nlen(caches) = \n 2\n\u001b[92mAll tests passed\n","output_type":"stream"}]},{"cell_type":"markdown","source":"LSTM - Single cell","metadata":{}},{"cell_type":"code","source":"# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: lstm_cell_forward\n\ndef lstm_cell_forward(xt, a_prev, c_prev, parameters):\n    \"\"\"\n    Implement a single forward step of the LSTM-cell as described in Figure (4)\n\n    Arguments:\n    xt -- your input data at timestep \"t\", numpy array of shape (n_x, m).\n    a_prev -- Hidden state at timestep \"t-1\", numpy array of shape (n_a, m)\n    c_prev -- Memory state at timestep \"t-1\", numpy array of shape (n_a, m)\n    parameters -- python dictionary containing:\n                        Wf -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)\n                        bf -- Bias of the forget gate, numpy array of shape (n_a, 1)\n                        Wi -- Weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)\n                        bi -- Bias of the update gate, numpy array of shape (n_a, 1)\n                        Wc -- Weight matrix of the first \"tanh\", numpy array of shape (n_a, n_a + n_x)\n                        bc --  Bias of the first \"tanh\", numpy array of shape (n_a, 1)\n                        Wo -- Weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)\n                        bo --  Bias of the output gate, numpy array of shape (n_a, 1)\n                        Wy -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n                        \n    Returns:\n    a_next -- next hidden state, of shape (n_a, m)\n    c_next -- next memory state, of shape (n_a, m)\n    yt_pred -- prediction at timestep \"t\", numpy array of shape (n_y, m)\n    cache -- tuple of values needed for the backward pass, contains (a_next, c_next, a_prev, c_prev, xt, parameters)\n       Note: ft/it/ot stand for the forget/update/output gates, cct stands for the candidate value (c tilde),\n          c stands for the cell state (memory)\n    \"\"\"\n\n    # Retrieve parameters from \"parameters\"\n    Wf = parameters[\"Wf\"] # forget gate weight\n    bf = parameters[\"bf\"]\n    Wi = parameters[\"Wi\"] # update gate weight (notice the variable name)\n    bi = parameters[\"bi\"] # (notice the variable name)\n    Wc = parameters[\"Wc\"] # candidate value weight\n    bc = parameters[\"bc\"]\n    Wo = parameters[\"Wo\"] # output gate weight\n    bo = parameters[\"bo\"]\n    Wy = parameters[\"Wy\"] # prediction weight\n    by = parameters[\"by\"]\n    \n    # Retrieve dimensions from shapes of xt and Wy\n    n_x, m = xt.shape\n    n_y, n_a = Wy.shape\n\n    ### START CODE HERE ###\n    # Concatenate a_prev and xt (≈1 line)\n    concat_var = np.concatenate([a_prev,xt])\n    \n    # Compute values for ft, it, cct, c_next, ot, a_next using the formulas given figure (4) (≈6 lines)\n    ft = sigmoid(np.dot(Wf,concat_var)+bf)\n    it = sigmoid(np.dot(Wi,concat_var)+bi)\n    ot = sigmoid(np.dot(Wo,concat_var)+bo)\n    cct = np.tanh(np.dot(Wc,concat_var)+bc)\n    c_next = it*cct+ft*c_prev\n    a_next = ot*np.tanh(c_next)\n    \n    # Compute prediction of the LSTM cell (≈1 line)\n    yt_pred = softmax(np.dot(Wy,a_next)+by)\n    \n    ### END CODE HERE ###\n\n    # store values needed for backward propagation in cache\n    cache = (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters)\n\n    return a_next, c_next, yt_pred, cache\n","metadata":{"execution":{"iopub.status.busy":"2024-02-08T17:21:25.362541Z","iopub.execute_input":"2024-02-08T17:21:25.363047Z","iopub.status.idle":"2024-02-08T17:21:25.377760Z","shell.execute_reply.started":"2024-02-08T17:21:25.363013Z","shell.execute_reply":"2024-02-08T17:21:25.376435Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"np.random.seed(1)\nxt_tmp = np.random.randn(3, 10)\na_prev_tmp = np.random.randn(5, 10)\nc_prev_tmp = np.random.randn(5, 10)\nparameters_tmp = {}\nparameters_tmp['Wf'] = np.random.randn(5, 5 + 3)\nparameters_tmp['bf'] = np.random.randn(5, 1)\nparameters_tmp['Wi'] = np.random.randn(5, 5 + 3)\nparameters_tmp['bi'] = np.random.randn(5, 1)\nparameters_tmp['Wo'] = np.random.randn(5, 5 + 3)\nparameters_tmp['bo'] = np.random.randn(5, 1)\nparameters_tmp['Wc'] = np.random.randn(5, 5 + 3)\nparameters_tmp['bc'] = np.random.randn(5, 1)\nparameters_tmp['Wy'] = np.random.randn(2, 5)\nparameters_tmp['by'] = np.random.randn(2, 1)\n\na_next_tmp, c_next_tmp, yt_tmp, cache_tmp = lstm_cell_forward(xt_tmp, a_prev_tmp, c_prev_tmp, parameters_tmp)\n\nprint(\"a_next[4] = \\n\", a_next_tmp[4])\nprint(\"a_next.shape = \", a_next_tmp.shape)\nprint(\"c_next[2] = \\n\", c_next_tmp[2])\nprint(\"c_next.shape = \", c_next_tmp.shape)\nprint(\"yt[1] =\", yt_tmp[1])\nprint(\"yt.shape = \", yt_tmp.shape)\nprint(\"cache[1][3] =\\n\", cache_tmp[1][3])\nprint(\"len(cache) = \", len(cache_tmp))\n\n# UNIT TEST\nlstm_cell_forward_test(lstm_cell_forward)","metadata":{"execution":{"iopub.status.busy":"2024-02-08T17:21:26.153954Z","iopub.execute_input":"2024-02-08T17:21:26.154372Z","iopub.status.idle":"2024-02-08T17:21:26.171789Z","shell.execute_reply.started":"2024-02-08T17:21:26.154338Z","shell.execute_reply":"2024-02-08T17:21:26.170437Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"a_next[4] = \n [-0.66408471  0.0036921   0.02088357  0.22834167 -0.85575339  0.00138482\n  0.76566531  0.34631421 -0.00215674  0.43827275]\na_next.shape =  (5, 10)\nc_next[2] = \n [ 0.63267805  1.00570849  0.35504474  0.20690913 -1.64566718  0.11832942\n  0.76449811 -0.0981561  -0.74348425 -0.26810932]\nc_next.shape =  (5, 10)\nyt[1] = [0.79913913 0.15986619 0.22412122 0.15606108 0.97057211 0.31146381\n 0.00943007 0.12666353 0.39380172 0.07828381]\nyt.shape =  (2, 10)\ncache[1][3] =\n [-0.16263996  1.03729328  0.72938082 -0.54101719  0.02752074 -0.30821874\n  0.07651101 -1.03752894  1.41219977 -0.37647422]\nlen(cache) =  10\n\u001b[92mAll tests passed\n","output_type":"stream"}]},{"cell_type":"markdown","source":"LSTM Network forward Propogation","metadata":{}},{"cell_type":"code","source":"# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: lstm_forward\n\ndef lstm_forward(x, a0, parameters):\n    \"\"\"\n    Implement the forward propagation of the recurrent neural network using an LSTM-cell described in Figure (4).\n\n    Arguments:\n    x -- Input data for every time-step, of shape (n_x, m, T_x).\n    a0 -- Initial hidden state, of shape (n_a, m)\n    parameters -- python dictionary containing:\n                        Wf -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)\n                        bf -- Bias of the forget gate, numpy array of shape (n_a, 1)\n                        Wi -- Weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)\n                        bi -- Bias of the update gate, numpy array of shape (n_a, 1)\n                        Wc -- Weight matrix of the first \"tanh\", numpy array of shape (n_a, n_a + n_x)\n                        bc -- Bias of the first \"tanh\", numpy array of shape (n_a, 1)\n                        Wo -- Weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)\n                        bo -- Bias of the output gate, numpy array of shape (n_a, 1)\n                        Wy -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n                        \n    Returns:\n    a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)\n    y -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)\n    c -- The value of the cell state, numpy array of shape (n_a, m, T_x)\n    caches -- tuple of values needed for the backward pass, contains (list of all the caches, x)\n    \"\"\"\n    # Initialize \"caches\", which will track the list of all the caches\n    caches = []\n    \n    ### START CODE HERE ###\n    #Wy = parameters['Wy'] # Save parameters in local variables in case you want to use Wy instead of parameters['Wy']\n    # Retrieve dimensions from shapes of x and parameters['Wy'] (≈2 lines)\n    \n    n_x,m,T_x = x.shape\n    n_y,n_a = parameters['Wy'].shape\n    \n    # initialize \"a\", \"c\" and \"y\" with zeros (≈3 lines)\n    a = np.zeros((n_a, m, T_x))\n    c = np.zeros((n_a, m, T_x))\n    y = np.zeros((n_y, m, T_x))\n    \n    \n    # Initialize a_next and c_next (≈2 lines)\n    a_next = a0\n    c_next = np.zeros((n_a,m))\n    \n    # loop over all time-steps\n    for t in range(T_x):\n        # Get the 2D slice 'xt' from the 3D input 'x' at time step 't'\n        xt = x[:,:,t]\n        # Update next hidden state, next memory state, compute the prediction, get the cache (≈1 line)\n        \n        a_next, c_next, yt, cache = lstm_cell_forward(xt, a_next, c_next, parameters)\n\n        # Save the value of the new \"next\" hidden state in a (≈1 line)\n        \n        a[:,:,t] = a_next\n        \n        # Save the value of the next cell state (≈1 line)\n        \n        c[:,:,t] = c_next\n        \n        # Save the value of the prediction in y (≈1 line)\n        \n        y[:,:,t] = yt\n        \n        # Append the cache into caches (≈1 line)\n        \n        caches.append(cache)\n        ### END CODE HERE ###\n    \n    # store values needed for backward propagation in cache\n    caches = (caches, x)\n\n    return a, y, c, caches","metadata":{"execution":{"iopub.status.busy":"2024-02-08T17:42:32.295757Z","iopub.execute_input":"2024-02-08T17:42:32.296242Z","iopub.status.idle":"2024-02-08T17:42:32.312851Z","shell.execute_reply.started":"2024-02-08T17:42:32.296209Z","shell.execute_reply":"2024-02-08T17:42:32.310908Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"np.random.seed(1)\nx_tmp = np.random.randn(3, 10, 7)\na0_tmp = np.random.randn(5, 10)\nparameters_tmp = {}\nparameters_tmp['Wf'] = np.random.randn(5, 5 + 3)\nparameters_tmp['bf'] = np.random.randn(5, 1)\nparameters_tmp['Wi'] = np.random.randn(5, 5 + 3)\nparameters_tmp['bi']= np.random.randn(5, 1)\nparameters_tmp['Wo'] = np.random.randn(5, 5 + 3)\nparameters_tmp['bo'] = np.random.randn(5, 1)\nparameters_tmp['Wc'] = np.random.randn(5, 5 + 3)\nparameters_tmp['bc'] = np.random.randn(5, 1)\nparameters_tmp['Wy'] = np.random.randn(2, 5)\nparameters_tmp['by'] = np.random.randn(2, 1)\n\na_tmp, y_tmp, c_tmp, caches_tmp = lstm_forward(x_tmp, a0_tmp, parameters_tmp)\nprint(\"a[4][3][6] = \", a_tmp[4][3][6])\nprint(\"a.shape = \", a_tmp.shape)\nprint(\"y[1][4][3] =\", y_tmp[1][4][3])\nprint(\"y.shape = \", y_tmp.shape)\nprint(\"caches[1][1][1] =\\n\", caches_tmp[1][1][1])\nprint(\"c[1][2][1]\", c_tmp[1][2][1])\nprint(\"len(caches) = \", len(caches_tmp))\n\n# UNIT TEST    \nlstm_forward_test(lstm_forward)","metadata":{"execution":{"iopub.status.busy":"2024-02-08T17:42:33.388262Z","iopub.execute_input":"2024-02-08T17:42:33.389182Z","iopub.status.idle":"2024-02-08T17:42:33.408274Z","shell.execute_reply.started":"2024-02-08T17:42:33.389141Z","shell.execute_reply":"2024-02-08T17:42:33.406613Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"a[4][3][6] =  0.1721177675329167\na.shape =  (5, 10, 7)\ny[1][4][3] = 0.9508734618501101\ny.shape =  (2, 10, 7)\ncaches[1][1][1] =\n [ 0.82797464  0.23009474  0.76201118 -0.22232814 -0.20075807  0.18656139\n  0.41005165]\nc[1][2][1] -0.8555449167181983\nlen(caches) =  2\n\u001b[92mAll tests passed\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}